 有些鸡零狗碎，但实践中遇到的问题
 # transformer
## AutoModel、AutoModelForCausalLM、LlamaForCausalLM、BloomForCausalLM等有什么区别


## tokenizer知识总结

## 模型参数量、训练、推理所需GPU内存大小
1. 模型参数量
transformer有l层，每层由self-attention模块和MLP模块
self-attention模块 Q K V 和输出权重矩阵O（多头注意力concat后经过一个线性层才输出）共四个参数矩阵，中Wq、Wk、Wv、Wo维度都是[h,h]，bias部分维度为b
则这部分参数量为(3+1)h^2^ + (3+1)h   (3不用乘多头n是因为输入h 输出 h/head_nums 最后在拼接起来抵消了)
MLP模块两层线性层，h到4h，4h到h，参数量为8h^2^ + 5h
因此每层共12h^2^,l层共12h^2^l
- 中间激活值与输入数据的大小（批次大小和序列长度）是成正相关的，随着批次大小和序列长度的增大，中间激活占用的显存会同步增大。当我们训练神经网络遇到显存不足OOM（Out Of Memory）问题时，通常会尝试减小批次大小来避免显存不足的问题，这种方式减少的其实是中间激活占用的显存，而不是模型参数、梯度和优化器的显存。
2. 训练成本
fp16 2字节，优化器避免溢出，采用fp32
参数2 梯度2 优化器（参数4 梯度4 两个动量4*2）共20；
模型本身参数为nB，则训练成本为20nG
3. 推理成本
只有参数 fp16
模型本身参数为nB，则训练成本为2nG

## 由前一个问题衍生出的：调整内存相关的超参数，背后的原理